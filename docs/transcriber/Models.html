<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>transcriber.Models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>transcriber.Models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import whisper
import torch

from typing import Dict
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq


class Models():
    &#34;&#34;&#34;
    Provides a class for loading and managing Whisper speech recognition models.
    ----------------------------------------------------------------------------
    Parameters
    ----------
    model_tpye: str
        The model_type to use. Default: &#34;pretrained&#34;
    model_size: str
        The size of the model to use. Default: &#34;small&#34;
    device: str
        The device to use for PyTorch operations. Default: None
    &#34;&#34;&#34;
    def __init__(self, model_type: str=&#34;pretrained&#34;, model_size: str=&#34;small&#34;, device: str=None):
        self.model_type = model_type
        self.model_size = model_size
        
        self.device = device
    
    def load_vanilla(self):
        &#34;&#34;&#34;Loads the stock Whisper model of the specified size.
    
        This method initializes the speech recognition model by loading the pre-trained Whisper model of the specified size. 
        The loaded model is stored in the `speech_model` attribute.
        &#34;&#34;&#34;
        model = whisper.load_model(self.model_size)
        
        self.speech_model = model
        
        print(&#34;Loaded stock whisper model...&#34;)
        
    def load_pretrained(self):
        &#34;&#34;&#34;Loads a pre-trained Whisper model for speech recognition.
    
        This method initializes the speech recognition model by loading a pre-trained Whisper model of the specified size. 
        The loaded model and processor are stored in the `speech_model` and `processor` attributes, respectively.
        &#34;&#34;&#34;
        model = AutoModelForSpeechSeq2Seq.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;).to(self.device)
        processor = AutoProcessor.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;, language=&#34;german&#34;, task=&#34;transcribe&#34;)
        model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=&#34;de&#34;, task=&#34;transcribe&#34;)
        
        self.speech_model = model
        self.processor = processor
        
        print(&#34;Loaded pretrained whisper model...&#34;)
    
    def load(self):
        &#34;&#34;&#34;Loads the appropriate Whisper model based on the specified model type.
    
        This method checks the `model_type` attribute and loads either the stock Whisper model or a pre-trained Whisper model. 
        If the `model_type` is &#34;pretrained&#34;, it loads the pre-trained model using the `load_pretrained()` method. 
        If the `model_type` is &#34;vanilla&#34;, it loads the stock Whisper model using the `load_vanilla()` method. 
        If the `model_type` is neither &#34;pretrained&#34; nor &#34;vanilla&#34;, it raises a `ValueError`.
        
        Raises:
            ValueError: If the provided `model_type` is not supported.
        &#34;&#34;&#34;
        if self.model_type == &#34;pretrained&#34;:
            self.load_pretrained()
        elif self.model_type == &#34;vanilla&#34;:
            self.load_vanilla()
        else:
            raise ValueError(&#34;Model type not supported.&#34;)
        
    def check_params(self, model_type: str, model_size: str, device: str=None):
        &#34;&#34;&#34;Checks the validity of the provided model parameters and sets the appropriate values.
    
        This method checks the provided `model_type` and `model_size` parameters to ensure they are valid. 
        If the provided values are not valid, it sets the parameters to the default values. 
        It also sets the `device` parameter based on the provided value, falling back to CPU if the provided device is not available.
    
        The method prints out the final values of the model parameters for debugging purposes.
    
        Args:
            model_type (str): The type of the Whisper model to use. Can be either &#34;vanilla&#34; or &#34;pretrained&#34;.
            model_size (str): The size of the Whisper model to use. The available sizes depend on the `model_type`.
            device (str, optional): The device to use for the Whisper model. Can be &#34;cpu&#34;, &#34;cuda&#34;, or &#34;mps&#34;. If not provided, the method will attempt to set the device automatically.
        &#34;&#34;&#34;
        available_model_types = [&#34;vanilla&#34;, &#34;pretrained&#34;]
        available_model_sizes = {&#34;vanilla&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
                                 &#34;pretrained&#34;: [&#34;small&#34;, &#34;medium&#34;, &#34;large&#34;]}
        
        
        
        self.model_type = model_type if model_type in available_model_types else &#34;vanilla&#34;
        self.model_size = model_size if model_size in available_model_sizes[self.model_type] else &#34;small&#34;
        self.model_size = &#34;large-v2&#34; if model_size == &#34;large&#34; and self.model_type == &#34;pretrained&#34; else model_size
            
        if model_type not in available_model_types:
            print(f&#34;Model type not supported. Defaulting to {self.model_type}.&#34;)
        
        if model_size not in available_model_sizes[self.model_type]:
            print(f&#34;Model size not supported. Defaulting to {self.model_size}.&#34;)
        
        if device is None:
            self.device = self.set_device()
        elif device in [&#34;cpu&#34;, &#34;cuda&#34;, &#34;mps&#34;]:
            try:
                self.device = torch.device(device)
            except Exception as e:
                print(e)
                self.device = torch.device(&#34;cpu&#34;)
        else:
            self.device = torch.device(&#34;cpu&#34;)
            
        print(f&#34;Checked model parameters: \n\
            model_type: {self.model_type}\n\
                model_size: {self.model_size}\n\
                    device: {self.device}&#34;)
        
    @staticmethod
    def get_models():
        &#34;&#34;&#34;Prints a dictionary of available Whisper model types and sizes.
    
        The dictionary contains two keys: &#34;vanilla whisper&#34; and &#34;pretrained whisper&#34;. Each key maps to a list of available model sizes for that model type.
    
        This function is used to provide information about the Whisper models that can be used in the application.
        &#34;&#34;&#34;
        models: Dict = {
            &#34;vanilla whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
            &#34;pretrained whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large-v2&#34;],
        }
        print(&#34;Available models:&#34;)
        for model_type in models:
            print(f&#34;{model_type}: {models[model_type]}&#34;)
    
    @staticmethod
    def set_device() -&gt; torch.device:
        &#34;&#34;&#34;Determines the appropriate device to use for PyTorch operations, prioritizing GPU and MPS (Apple Silicon) devices if available, and falling back to CPU if neither is available.
    
        Returns:
            torch.device: The device to use for PyTorch operations.
        &#34;&#34;&#34;
        if torch.cuda.is_available():
            device = torch.device(&#34;cuda&#34;)
        elif torch.backends.mps.is_available():
            device = torch.device(&#34;mps&#34;)
        else:
            device = torch.device(&#34;cpu&#34;)
            
        return device</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="transcriber.Models.Models"><code class="flex name class">
<span>class <span class="ident">Models</span></span>
<span>(</span><span>model_type: str = 'pretrained', model_size: str = 'small', device: str = None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="provides-a-class-for-loading-and-managing-whisper-speech-recognition-models">Provides a class for loading and managing Whisper speech recognition models.</h2>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_tpye</code></strong> :&ensp;<code>str</code></dt>
<dd>The model_type to use. Default: "pretrained"</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of the model to use. Default: "small"</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>The device to use for PyTorch operations. Default: None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Models():
    &#34;&#34;&#34;
    Provides a class for loading and managing Whisper speech recognition models.
    ----------------------------------------------------------------------------
    Parameters
    ----------
    model_tpye: str
        The model_type to use. Default: &#34;pretrained&#34;
    model_size: str
        The size of the model to use. Default: &#34;small&#34;
    device: str
        The device to use for PyTorch operations. Default: None
    &#34;&#34;&#34;
    def __init__(self, model_type: str=&#34;pretrained&#34;, model_size: str=&#34;small&#34;, device: str=None):
        self.model_type = model_type
        self.model_size = model_size
        
        self.device = device
    
    def load_vanilla(self):
        &#34;&#34;&#34;Loads the stock Whisper model of the specified size.
    
        This method initializes the speech recognition model by loading the pre-trained Whisper model of the specified size. 
        The loaded model is stored in the `speech_model` attribute.
        &#34;&#34;&#34;
        model = whisper.load_model(self.model_size)
        
        self.speech_model = model
        
        print(&#34;Loaded stock whisper model...&#34;)
        
    def load_pretrained(self):
        &#34;&#34;&#34;Loads a pre-trained Whisper model for speech recognition.
    
        This method initializes the speech recognition model by loading a pre-trained Whisper model of the specified size. 
        The loaded model and processor are stored in the `speech_model` and `processor` attributes, respectively.
        &#34;&#34;&#34;
        model = AutoModelForSpeechSeq2Seq.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;).to(self.device)
        processor = AutoProcessor.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;, language=&#34;german&#34;, task=&#34;transcribe&#34;)
        model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=&#34;de&#34;, task=&#34;transcribe&#34;)
        
        self.speech_model = model
        self.processor = processor
        
        print(&#34;Loaded pretrained whisper model...&#34;)
    
    def load(self):
        &#34;&#34;&#34;Loads the appropriate Whisper model based on the specified model type.
    
        This method checks the `model_type` attribute and loads either the stock Whisper model or a pre-trained Whisper model. 
        If the `model_type` is &#34;pretrained&#34;, it loads the pre-trained model using the `load_pretrained()` method. 
        If the `model_type` is &#34;vanilla&#34;, it loads the stock Whisper model using the `load_vanilla()` method. 
        If the `model_type` is neither &#34;pretrained&#34; nor &#34;vanilla&#34;, it raises a `ValueError`.
        
        Raises:
            ValueError: If the provided `model_type` is not supported.
        &#34;&#34;&#34;
        if self.model_type == &#34;pretrained&#34;:
            self.load_pretrained()
        elif self.model_type == &#34;vanilla&#34;:
            self.load_vanilla()
        else:
            raise ValueError(&#34;Model type not supported.&#34;)
        
    def check_params(self, model_type: str, model_size: str, device: str=None):
        &#34;&#34;&#34;Checks the validity of the provided model parameters and sets the appropriate values.
    
        This method checks the provided `model_type` and `model_size` parameters to ensure they are valid. 
        If the provided values are not valid, it sets the parameters to the default values. 
        It also sets the `device` parameter based on the provided value, falling back to CPU if the provided device is not available.
    
        The method prints out the final values of the model parameters for debugging purposes.
    
        Args:
            model_type (str): The type of the Whisper model to use. Can be either &#34;vanilla&#34; or &#34;pretrained&#34;.
            model_size (str): The size of the Whisper model to use. The available sizes depend on the `model_type`.
            device (str, optional): The device to use for the Whisper model. Can be &#34;cpu&#34;, &#34;cuda&#34;, or &#34;mps&#34;. If not provided, the method will attempt to set the device automatically.
        &#34;&#34;&#34;
        available_model_types = [&#34;vanilla&#34;, &#34;pretrained&#34;]
        available_model_sizes = {&#34;vanilla&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
                                 &#34;pretrained&#34;: [&#34;small&#34;, &#34;medium&#34;, &#34;large&#34;]}
        
        
        
        self.model_type = model_type if model_type in available_model_types else &#34;vanilla&#34;
        self.model_size = model_size if model_size in available_model_sizes[self.model_type] else &#34;small&#34;
        self.model_size = &#34;large-v2&#34; if model_size == &#34;large&#34; and self.model_type == &#34;pretrained&#34; else model_size
            
        if model_type not in available_model_types:
            print(f&#34;Model type not supported. Defaulting to {self.model_type}.&#34;)
        
        if model_size not in available_model_sizes[self.model_type]:
            print(f&#34;Model size not supported. Defaulting to {self.model_size}.&#34;)
        
        if device is None:
            self.device = self.set_device()
        elif device in [&#34;cpu&#34;, &#34;cuda&#34;, &#34;mps&#34;]:
            try:
                self.device = torch.device(device)
            except Exception as e:
                print(e)
                self.device = torch.device(&#34;cpu&#34;)
        else:
            self.device = torch.device(&#34;cpu&#34;)
            
        print(f&#34;Checked model parameters: \n\
            model_type: {self.model_type}\n\
                model_size: {self.model_size}\n\
                    device: {self.device}&#34;)
        
    @staticmethod
    def get_models():
        &#34;&#34;&#34;Prints a dictionary of available Whisper model types and sizes.
    
        The dictionary contains two keys: &#34;vanilla whisper&#34; and &#34;pretrained whisper&#34;. Each key maps to a list of available model sizes for that model type.
    
        This function is used to provide information about the Whisper models that can be used in the application.
        &#34;&#34;&#34;
        models: Dict = {
            &#34;vanilla whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
            &#34;pretrained whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large-v2&#34;],
        }
        print(&#34;Available models:&#34;)
        for model_type in models:
            print(f&#34;{model_type}: {models[model_type]}&#34;)
    
    @staticmethod
    def set_device() -&gt; torch.device:
        &#34;&#34;&#34;Determines the appropriate device to use for PyTorch operations, prioritizing GPU and MPS (Apple Silicon) devices if available, and falling back to CPU if neither is available.
    
        Returns:
            torch.device: The device to use for PyTorch operations.
        &#34;&#34;&#34;
        if torch.cuda.is_available():
            device = torch.device(&#34;cuda&#34;)
        elif torch.backends.mps.is_available():
            device = torch.device(&#34;mps&#34;)
        else:
            device = torch.device(&#34;cpu&#34;)
            
        return device</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="transcriber.Inference.Inference" href="Inference.html#transcriber.Inference.Inference">Inference</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="transcriber.Models.Models.get_models"><code class="name flex">
<span>def <span class="ident">get_models</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Prints a dictionary of available Whisper model types and sizes.</p>
<p>The dictionary contains two keys: "vanilla whisper" and "pretrained whisper". Each key maps to a list of available model sizes for that model type.</p>
<p>This function is used to provide information about the Whisper models that can be used in the application.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_models():
    &#34;&#34;&#34;Prints a dictionary of available Whisper model types and sizes.

    The dictionary contains two keys: &#34;vanilla whisper&#34; and &#34;pretrained whisper&#34;. Each key maps to a list of available model sizes for that model type.

    This function is used to provide information about the Whisper models that can be used in the application.
    &#34;&#34;&#34;
    models: Dict = {
        &#34;vanilla whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
        &#34;pretrained whisper&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large-v2&#34;],
    }
    print(&#34;Available models:&#34;)
    for model_type in models:
        print(f&#34;{model_type}: {models[model_type]}&#34;)</code></pre>
</details>
</dd>
<dt id="transcriber.Models.Models.set_device"><code class="name flex">
<span>def <span class="ident">set_device</span></span>(<span>) ‑> torch.device</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the appropriate device to use for PyTorch operations, prioritizing GPU and MPS (Apple Silicon) devices if available, and falling back to CPU if neither is available.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.device</code></dt>
<dd>The device to use for PyTorch operations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def set_device() -&gt; torch.device:
    &#34;&#34;&#34;Determines the appropriate device to use for PyTorch operations, prioritizing GPU and MPS (Apple Silicon) devices if available, and falling back to CPU if neither is available.

    Returns:
        torch.device: The device to use for PyTorch operations.
    &#34;&#34;&#34;
    if torch.cuda.is_available():
        device = torch.device(&#34;cuda&#34;)
    elif torch.backends.mps.is_available():
        device = torch.device(&#34;mps&#34;)
    else:
        device = torch.device(&#34;cpu&#34;)
        
    return device</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="transcriber.Models.Models.check_params"><code class="name flex">
<span>def <span class="ident">check_params</span></span>(<span>self, model_type: str, model_size: str, device: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks the validity of the provided model parameters and sets the appropriate values.</p>
<p>This method checks the provided <code>model_type</code> and <code>model_size</code> parameters to ensure they are valid.
If the provided values are not valid, it sets the parameters to the default values.
It also sets the <code>device</code> parameter based on the provided value, falling back to CPU if the provided device is not available.</p>
<p>The method prints out the final values of the model parameters for debugging purposes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the Whisper model to use. Can be either "vanilla" or "pretrained".</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of the Whisper model to use. The available sizes depend on the <code>model_type</code>.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The device to use for the Whisper model. Can be "cpu", "cuda", or "mps". If not provided, the method will attempt to set the device automatically.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_params(self, model_type: str, model_size: str, device: str=None):
    &#34;&#34;&#34;Checks the validity of the provided model parameters and sets the appropriate values.

    This method checks the provided `model_type` and `model_size` parameters to ensure they are valid. 
    If the provided values are not valid, it sets the parameters to the default values. 
    It also sets the `device` parameter based on the provided value, falling back to CPU if the provided device is not available.

    The method prints out the final values of the model parameters for debugging purposes.

    Args:
        model_type (str): The type of the Whisper model to use. Can be either &#34;vanilla&#34; or &#34;pretrained&#34;.
        model_size (str): The size of the Whisper model to use. The available sizes depend on the `model_type`.
        device (str, optional): The device to use for the Whisper model. Can be &#34;cpu&#34;, &#34;cuda&#34;, or &#34;mps&#34;. If not provided, the method will attempt to set the device automatically.
    &#34;&#34;&#34;
    available_model_types = [&#34;vanilla&#34;, &#34;pretrained&#34;]
    available_model_sizes = {&#34;vanilla&#34;: [&#34;base&#34;, &#34;small&#34;, &#34;medium&#34;, &#34;large&#34;],
                             &#34;pretrained&#34;: [&#34;small&#34;, &#34;medium&#34;, &#34;large&#34;]}
    
    
    
    self.model_type = model_type if model_type in available_model_types else &#34;vanilla&#34;
    self.model_size = model_size if model_size in available_model_sizes[self.model_type] else &#34;small&#34;
    self.model_size = &#34;large-v2&#34; if model_size == &#34;large&#34; and self.model_type == &#34;pretrained&#34; else model_size
        
    if model_type not in available_model_types:
        print(f&#34;Model type not supported. Defaulting to {self.model_type}.&#34;)
    
    if model_size not in available_model_sizes[self.model_type]:
        print(f&#34;Model size not supported. Defaulting to {self.model_size}.&#34;)
    
    if device is None:
        self.device = self.set_device()
    elif device in [&#34;cpu&#34;, &#34;cuda&#34;, &#34;mps&#34;]:
        try:
            self.device = torch.device(device)
        except Exception as e:
            print(e)
            self.device = torch.device(&#34;cpu&#34;)
    else:
        self.device = torch.device(&#34;cpu&#34;)
        
    print(f&#34;Checked model parameters: \n\
        model_type: {self.model_type}\n\
            model_size: {self.model_size}\n\
                device: {self.device}&#34;)</code></pre>
</details>
</dd>
<dt id="transcriber.Models.Models.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the appropriate Whisper model based on the specified model type.</p>
<p>This method checks the <code>model_type</code> attribute and loads either the stock Whisper model or a pre-trained Whisper model.
If the <code>model_type</code> is "pretrained", it loads the pre-trained model using the <code>load_pretrained()</code> method.
If the <code>model_type</code> is "vanilla", it loads the stock Whisper model using the <code>load_vanilla()</code> method.
If the <code>model_type</code> is neither "pretrained" nor "vanilla", it raises a <code>ValueError</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided <code>model_type</code> is not supported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;Loads the appropriate Whisper model based on the specified model type.

    This method checks the `model_type` attribute and loads either the stock Whisper model or a pre-trained Whisper model. 
    If the `model_type` is &#34;pretrained&#34;, it loads the pre-trained model using the `load_pretrained()` method. 
    If the `model_type` is &#34;vanilla&#34;, it loads the stock Whisper model using the `load_vanilla()` method. 
    If the `model_type` is neither &#34;pretrained&#34; nor &#34;vanilla&#34;, it raises a `ValueError`.
    
    Raises:
        ValueError: If the provided `model_type` is not supported.
    &#34;&#34;&#34;
    if self.model_type == &#34;pretrained&#34;:
        self.load_pretrained()
    elif self.model_type == &#34;vanilla&#34;:
        self.load_vanilla()
    else:
        raise ValueError(&#34;Model type not supported.&#34;)</code></pre>
</details>
</dd>
<dt id="transcriber.Models.Models.load_pretrained"><code class="name flex">
<span>def <span class="ident">load_pretrained</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a pre-trained Whisper model for speech recognition.</p>
<p>This method initializes the speech recognition model by loading a pre-trained Whisper model of the specified size.
The loaded model and processor are stored in the <code>speech_model</code> and <code>processor</code> attributes, respectively.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_pretrained(self):
    &#34;&#34;&#34;Loads a pre-trained Whisper model for speech recognition.

    This method initializes the speech recognition model by loading a pre-trained Whisper model of the specified size. 
    The loaded model and processor are stored in the `speech_model` and `processor` attributes, respectively.
    &#34;&#34;&#34;
    model = AutoModelForSpeechSeq2Seq.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;).to(self.device)
    processor = AutoProcessor.from_pretrained(f&#34;bofenghuang/whisper-{self.model_size}-cv11-german&#34;, language=&#34;german&#34;, task=&#34;transcribe&#34;)
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=&#34;de&#34;, task=&#34;transcribe&#34;)
    
    self.speech_model = model
    self.processor = processor
    
    print(&#34;Loaded pretrained whisper model...&#34;)</code></pre>
</details>
</dd>
<dt id="transcriber.Models.Models.load_vanilla"><code class="name flex">
<span>def <span class="ident">load_vanilla</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the stock Whisper model of the specified size.</p>
<p>This method initializes the speech recognition model by loading the pre-trained Whisper model of the specified size.
The loaded model is stored in the <code>speech_model</code> attribute.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_vanilla(self):
    &#34;&#34;&#34;Loads the stock Whisper model of the specified size.

    This method initializes the speech recognition model by loading the pre-trained Whisper model of the specified size. 
    The loaded model is stored in the `speech_model` attribute.
    &#34;&#34;&#34;
    model = whisper.load_model(self.model_size)
    
    self.speech_model = model
    
    print(&#34;Loaded stock whisper model...&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="transcriber" href="index.html">transcriber</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="transcriber.Models.Models" href="#transcriber.Models.Models">Models</a></code></h4>
<ul class="two-column">
<li><code><a title="transcriber.Models.Models.check_params" href="#transcriber.Models.Models.check_params">check_params</a></code></li>
<li><code><a title="transcriber.Models.Models.get_models" href="#transcriber.Models.Models.get_models">get_models</a></code></li>
<li><code><a title="transcriber.Models.Models.load" href="#transcriber.Models.Models.load">load</a></code></li>
<li><code><a title="transcriber.Models.Models.load_pretrained" href="#transcriber.Models.Models.load_pretrained">load_pretrained</a></code></li>
<li><code><a title="transcriber.Models.Models.load_vanilla" href="#transcriber.Models.Models.load_vanilla">load_vanilla</a></code></li>
<li><code><a title="transcriber.Models.Models.set_device" href="#transcriber.Models.Models.set_device">set_device</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>